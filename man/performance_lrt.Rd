% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/performance_lrt.R
\name{performance_lrt}
\alias{performance_lrt}
\title{Likelihood-Ratio-Test for Model Comparison}
\usage{
performance_lrt(...)
}
\arguments{
\item{...}{Multiple model objects.}
}
\value{
A data frame, based on the results from \code{anova()}.
}
\description{
Compute Likelihood-Ratio-Test (LRT) for model comparison, which tests which model is a better (more likely) explanation of the data. Likelihood-Ratio-Test (LRT) gives usually somewhat close results (if not equivalent) to the commonly used F-test (obtained by default with \code{anova(...)}). Maximum likelihood tests make stronger assumptions than method of moments tests like the F-test, and in turn are more efficient.
\itemize{
\item For regression models, this is similar to \code{anova(..., test="LRT")} or \code{lmtest::lrtest(...)}, with one major difference: all models are tested against the same reference model (the first one), instead of testing each model to its previous neighbour.
\item For \code{lavaan} models (SEM, CFA), the function calls \code{lavaan::lavTestLRT()}.
}
}
\details{
This only makes statistical sense if the models are nested. It is conventional to list the models from smallest to largest, but this is up to the user. The output shows the tests of the models against one another in the order specified.
}
\examples{
# Regression Models
m1 <- lm(mpg ~ wt + cyl, data = mtcars)
m2 <- lm(mpg ~ wt + cyl + gear, data = mtcars)
m3 <- lm(mpg ~ wt + cyl + gear + disp, data = mtcars)
performance_lrt(m1, m2, m3)

# Lavaan Models
if (require("lavaan")) {
  structure <- " visual  =~ x1 + x2 + x3
                 textual =~ x4 + x5 + x6
                 speed   =~ x7 + x8 + x9 "
  m1 <- lavaan::cfa(structure, data = HolzingerSwineford1939)

  structure <- " visual  =~ x2 + x3
                 textual =~ x4 + x6
                 speed   =~ x8 + x9 "
  m2 <- lavaan::cfa(structure, data = HolzingerSwineford1939)

  performance_lrt(m1, m2)
}
}
\seealso{
\code{\link[=compare_performance]{compare_performance()}} to compare performance of many different models.
}
